{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('valuedWords.txt', 'r', encoding='UTF-8') as f:\n",
    "    valuedWords = f.readlines()\n",
    "    valuedWords = [\n",
    "        x\n",
    "        for x in [x[:-1] if x != valuedWords[-1] else x for x in valuedWords]\n",
    "        if x\n",
    "    ]\n",
    "\n",
    "with open('result.json', 'r', encoding='UTF-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "with open('good_list.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "wordss = content.split()\n",
    "good_words = wordss\n",
    "\n",
    "\n",
    "\n",
    "messages_texts = []\n",
    "\n",
    "for message in data['messages']:\n",
    "    messages_texts.append(message['text'])\n",
    "\n",
    "words = []\n",
    "\n",
    "\n",
    "def add_words(text):\n",
    "    words.extend(text.split())\n",
    "\n",
    "\n",
    "for sentence in messages_texts:\n",
    "    if isinstance(sentence, str):\n",
    "        add_words(sentence)\n",
    "    elif isinstance(sentence, list):\n",
    "        for item in sentence:\n",
    "            if isinstance(item, str):\n",
    "                add_words(item)\n",
    "            elif isinstance(item, dict) and 'text' in item:\n",
    "                add_words(item['text'])\n",
    "\n",
    "\n",
    "def detect_spam(text):\n",
    "    text = text.lower()\n",
    "    if 'аааа' in text or 'аха' in text or 'яяя' in text or 'f[f' in text or 'axa' in text or 'еее' in text or 'ойой' in text or 'оооо' in text or 'блябля' in text or 'feulnaim' in text or 'спасспас' in text or 'пжпжпжпж' in text or 'ээээ' in text or 'susalarm' in text or 'мммм' in text or 'аахх' in text or 'http' in text or 'pрууp' in text or 'kфklkфsфоф' in text or 'дvхfvpsfcхs' in text or 'шзолдюиа' in text or 'sxdvgrextpa' in text or 'уууу' in text or 'фjъaячъ' in text or 'ййй' in text or len(text) > 23:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def remove_non_alpha(word):\n",
    "    return ''.join(char for char in word if char.isalpha())\n",
    "\n",
    "\n",
    "spam = []\n",
    "temp_words = []\n",
    "\n",
    "parsed_words = [remove_non_alpha(word) for word in words]\n",
    "\n",
    "givenWords = list(filter(None, parsed_words))\n",
    "\n",
    "givenWords = [x.lower() for x in givenWords[:]]\n",
    "\n",
    "for word in givenWords:\n",
    "    if not detect_spam(word):\n",
    "        temp_words.append(word)\n",
    "    else:\n",
    "        spam.append(word)\n",
    "\n",
    "givenWords = temp_words[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "малхотрыкумарамахешвари\n"
     ]
    }
   ],
   "source": [
    "maxx, maxelem = 0, ''\n",
    "\n",
    "for elem in givenWords:\n",
    "    if len(elem) > maxx:\n",
    "        maxx = len(elem[:])\n",
    "        maxelem = elem[:]\n",
    "\n",
    "print(maxx)\n",
    "print(maxelem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s, t):\n",
    "    m = len(s)\n",
    "    n = len(t)\n",
    "    d = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    for j in range(1, n + 1):\n",
    "        for i in range(1, m + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                d[i][j] = min(d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + 1)\n",
    "    return d[m][n]\n",
    "\n",
    "\n",
    "def correct_spelling(word, valued_words):\n",
    "    if word == '':\n",
    "        closest_word = ''\n",
    "    else:\n",
    "        distances = [(levenshtein_distance(word, w), w) for w in valued_words]\n",
    "        # print(sorted(distances))\n",
    "        closest_word = min(distances)[1]\n",
    "        handled_closest = f\"{word} -- {closest_word} -- {min(distances)[0]}/{len(closest_word)} -- {int((round((min(distances)[0] / len(closest_word)) * 100, 0)))}%\"\n",
    "        handled_word = f\"{word} -- {min(distances)[0]}/{len(closest_word)} -- {int((round((min(distances)[0] / len(closest_word)) * 100, 0)))}%    ----- {closest_word}\"\n",
    "        if int((round((min(distances)[0] / len(closest_word)) * 100, 0))) < 27:\n",
    "            return handled_closest, 0\n",
    "        return handled_word, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_list = []\n",
    "original_list = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for word in givenWords:\n",
    "    temp = correct_spelling(word, valuedWords)\n",
    "    if temp[1] or word in good_words:\n",
    "        original_list.append(temp[0])\n",
    "    else:\n",
    "        changed_list.append(temp[0])\n",
    "    \n",
    "    if counter % 100 == 0:\n",
    "        print(f'{counter} {\"▮\" * int(str((counter / len(givenWords)) * 100)[:1])}{\"▯\" * (10 - int(str((counter / len(givenWords)) * 100)[:1]))} {str((counter / len(givenWords)) * 100)[:5]}% {word}')\n",
    "    if counter == len(givenWords):\n",
    "        print(f'{counter} \"▮▮▮▮▮▮▮▮▮▮ 100%')\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('original_list.txt', 'w', encoding='UTF-8') as file:\n",
    "    file.write('Original list\\n')\n",
    "    file.write('original_word | distance | distance_% | closest_word\\n')\n",
    "    for elem in original_list:\n",
    "        file.write(elem + '\\n')\n",
    "    file.write(str(len(original_list)))\n",
    "\n",
    "with open('changed_list.txt', 'w', encoding='UTF-8') as file:\n",
    "    file.write('Changed list\\n')\n",
    "    file.write('original_word | closest_word | distance | distance_%\\n')\n",
    "    for elem in changed_list:\n",
    "        file.write(elem + '\\n')\n",
    "    file.write(str(len(changed_list)))\n",
    "\n",
    "with open('spam.txt', 'w', encoding='UTF-8') as file:\n",
    "    file.write('Spam list\\n')\n",
    "    for elem in spam:\n",
    "        file.write(elem + '\\n')\n",
    "    file.write(str(len(spam)))\n",
    "\n",
    "print(f'Original length: {len(original_list)}')\n",
    "\n",
    "print(f'Swear: {len(changed_list)}')\n",
    "\n",
    "print(f'Swear_%: {len(changed_list) / len(original_list)}')\n",
    "\n",
    "print(f'Spam: {len(spam)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
